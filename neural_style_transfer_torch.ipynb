{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pytube"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QpP2K9Pls7X",
        "outputId": "c3a635f8-f2ee-4035-a837-0e23b473653c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m799.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytube\n",
            "Successfully installed pytube-15.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "G2AGZ3pHLJ5B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "# import tensorflow as tf\n",
        "# from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "# import tensorflow_datasets as tfds\n",
        "# from tensorflow.keras import layers\n",
        "import time\n",
        "# import utils.utils as utils\n",
        "# Defining the global variables.\n",
        "IMAGE_SIZE = (224, 224)\n",
        "BATCH_SIZE = 8\n",
        "# Training for single epoch for time constraint.\n",
        "# Please use atleast 30 epochs to see good results.\n",
        "EPOCHS = 1\n",
        "# AUTOTUNE = tf.data.AUTOTUNE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "from torchsummary import summary\n",
        "from torchvision.models.feature_extraction import create_feature_extractor\n",
        "\n",
        "def resize(image, size):\n",
        "    image = F.interpolate(image.unsqueeze(0), size=size, mode=\"nearest\").squeeze(0)\n",
        "    return image\n",
        "def calc_mean_std(feat, eps=1e-5):\n",
        "    # eps is a small value added to the variance to avoid divide-by-zero.\n",
        "    size = feat.size()\n",
        "    assert (len(size) == 4)\n",
        "    N, C = size[:2]\n",
        "    feat_var = feat.view(N, C, -1).var(dim=2) + eps\n",
        "    feat_std = feat_var.sqrt().view(N, C, 1, 1)\n",
        "    feat_mean = feat.view(N, C, -1).mean(dim=2).view(N, C, 1, 1)\n",
        "    return feat_mean, feat_std\n",
        "\n",
        "\n",
        "def st_ada_in(style_features, content_cur_masked_features, content_prev_masked_features, alpha = 0.1):\n",
        "    # print('#', style_features.shape, content_cur_masked_features.shape, content_prev_masked_features.shape) #BxCxSxS\n",
        "\n",
        "    x_cur_mean, x_cur_std = calc_mean_std(content_cur_masked_features)\n",
        "    x_prev_mean, x_prev_std = calc_mean_std(content_prev_masked_features)\n",
        "\n",
        "    # print('#', type(alpha), type(x_cur_mean), type(x_prev_mean))\n",
        "    combined_mean = (1-alpha) * x_cur_mean + alpha * x_prev_mean\n",
        "    combined_std = (1-alpha) * x_cur_std + alpha * x_prev_std\n",
        "\n",
        "    style_mean, style_std = calc_mean_std(style_features)\n",
        "\n",
        "    result = style_std * (content_cur_masked_features - combined_mean) / combined_std + style_mean\n",
        "    # print('#', x_cur_mean.shape, result.shape)\n",
        "    return result\n",
        "\n",
        "def get_VGG_feature_extractor():\n",
        "    # vgg_layer_names = [\"block1_conv1\", \"block2_conv1\", \"block3_conv1\", \"block4_conv1\"]\n",
        "    # vgg_layer_names = [\"ReLU-2\", \"ReLU-7\", \"ReLU-12\", \"ReLU-21\"]\n",
        "    vgg_fixed = torchvision.models.vgg19(weights='IMAGENET1K_V1')\n",
        "    for param in vgg_fixed.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # train_nodes, eval_nodes = get_graph_node_names(vgg_fixed)\n",
        "    # print(train_nodes, eval_nodes)\n",
        "\n",
        "    # To specify the nodes you want to extract, you could select the final node\n",
        "    # that appears in each of the main layers:\n",
        "    return_nodes = {\n",
        "        # node_name: user-specified key for output dict\n",
        "        'features.2': 'layer1',\n",
        "        'features.7': 'layer2',\n",
        "        'features.12': 'layer3',\n",
        "        'features.21': 'layer4',\n",
        "    }\n",
        "\n",
        "    return create_feature_extractor(m, return_nodes=return_nodes)\n",
        "\n",
        "\n",
        "class SpatioTemporalTransferModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SpatioTemporalTransferModel, self).__init__()\n",
        "        self.vgg_feature_extractor = get_VGG_feature_extractor()\n",
        "\n",
        "        #define splatting blocks' layers\n",
        "        self.sb_11 = ConvBlock(in_channels=64, out_channels=8, kernel_size=3,  padding=\"same\", stride=2)\n",
        "        self.sb_12 = ConvBlock(in_channels=8, out_channels=8, kernel_size=3,  padding=\"same\", stride=1)\n",
        "        self.sb_13 = ConvBlock(in_channels=8, out_channels=8, kernel_size=3,  padding=\"same\", stride=1)\n",
        "        self.sb_1_short = ConvBlock(in_channels=128, out_channels=8, kernel_size=3,  padding=\"same\", stride=1)\n",
        "\n",
        "        self.sb_21 = ConvBlock(in_channels=8, out_channels=16, kernel_size=3,  padding=\"same\", stride=2)\n",
        "        self.sb_22 = ConvBlock(in_channels=16, out_channels=16, kernel_size=3,  padding=\"same\", stride=1)\n",
        "        self.sb_23 = ConvBlock(in_channels=16, out_channels=16, kernel_size=3,  padding=\"same\", stride=1)\n",
        "        self.sb_2_short = ConvBlock(in_channels=256, out_channels=16, kernel_size=3,  padding=\"same\", stride=1)\n",
        "\n",
        "        self.sb_31 = ConvBlock(in_channels=16, out_channels=32, kernel_size=3,  padding=\"same\", stride=2)\n",
        "        self.sb_32 = ConvBlock(in_channels=32, out_channels=32, kernel_size=3,  padding=\"same\", stride=1)\n",
        "        self.sb_33 = ConvBlock(in_channels=32, out_channels=32, kernel_size=3,  padding=\"same\", stride=1)\n",
        "        self.sb_3_short = ConvBlock(in_channels=512, out_channels=32, kernel_size=3,  padding=\"same\", stride=1)\n",
        "\n",
        "        self.l_1 = ConvBlock(in_channels=32, out_channels=64, kernel_size=3,  padding=\"same\", stride=2)\n",
        "        self.l_2 = ConvBlock(in_channels=64, out_channels=64, kernel_size=3,  padding=\"same\", stride=1)\n",
        "\n",
        "        self.gp_1 = ConvBlock(in_channels=64, out_channels=64, kernel_size=3,  padding=\"same\", stride=1)\n",
        "\n",
        "        self.gp_2 = ConvBlock(in_channels=64, out_channels=96, kernel_size=1,  padding=\"same\", stride=1) #TODO : activation none?\n",
        "        return\n",
        "\n",
        "    def forward(self, style, content, mask, content_prev_encoded, temp_content_prev_1, temp_content_prev_2, temp_content_prev_3, mask_prev):\n",
        "        # Encode the style and content image.\n",
        "\n",
        "        style_encoded_dict = self.vgg_feature_extractor(style)\n",
        "        style_encoded = [style_encoded_dict[f'layer{i}'] for i in range(1,5)]\n",
        "\n",
        "        content_encoded_dict = self.vgg_feature_extractor(content * mask)\n",
        "        content_encoded = [content_encoded_dict[f'layer{i}'] for i in range(1,5)]\n",
        "\n",
        "        # for c in content_encoded:\n",
        "        #     print(c.shape)\n",
        "            # print(c)\n",
        "            # break\n",
        "\n",
        "        # Compute the AdaIN target feature maps.\n",
        "        st_ada_in_outputs = []\n",
        "        # skip block1_conv1\n",
        "        skipCount = 1\n",
        "        count = 0\n",
        "\n",
        "        # print('!!!!!!', len(style_encoded), len(content_encoded), len(content_prev_encoded))\n",
        "\n",
        "        for encoded_features in zip(style_encoded, content_encoded, content_prev_encoded):\n",
        "            # print('! ', count)\n",
        "            if count < skipCount:\n",
        "                count += 1\n",
        "                continue\n",
        "            # TODO : manage prev features and consider mask shape\n",
        "            st_ada_in_output = st_ada_in(encoded_features[0], encoded_features[1], encoded_features[2], alpha = 0.1)\n",
        "            st_ada_in_outputs.append(st_ada_in_output)\n",
        "\n",
        "        temp_content = self.sb_11(content_encoded[0]) #input is c_in\n",
        "        temp_style = self.sb_11(style_encoded[0]) #input is s_in\n",
        "        #TODO corresponding prev variables and mask\n",
        "        temp_content_stylized = st_ada_in(temp_style, temp_content, temp_content_prev_1, alpha = 0.1)\n",
        "        temp_content_prev_1 = temp_content.clone().detach()\n",
        "        temp_content_stylized = temp_content_stylized + self.sb_1_short(st_ada_in_outputs[0])\n",
        "        temp_content_stylized = self.sb_12(temp_content_stylized)\n",
        "        temp_content_stylized = self.sb_13(temp_content_stylized) #c_out\n",
        "        # print('here', temp_style.shape)\n",
        "        temp_style = self.sb_13(temp_style) #s_out\n",
        "\n",
        "        temp_content = self.sb_21(temp_content_stylized) #input is c_in\n",
        "        temp_style = self.sb_21(temp_style) #input is s_in\n",
        "        #TODO corresponding prev variables and mask\n",
        "        temp_content_stylized = st_ada_in(temp_style, temp_content, temp_content_prev_2, alpha = 0.1)\n",
        "        temp_content_prev_2 = temp_content.clone().detach()\n",
        "        temp_content_stylized = temp_content_stylized + self.sb_2_short(st_ada_in_outputs[1])\n",
        "        temp_content_stylized = self.sb_22(temp_content_stylized)\n",
        "        temp_content_stylized = self.sb_23(temp_content_stylized) #c_out\n",
        "        temp_style = self.sb_23(temp_style) #s_out\n",
        "\n",
        "        temp_content = self.sb_31(temp_content_stylized) #input is c_in\n",
        "        temp_style = self.sb_31(temp_style) #input is s_in\n",
        "        #TODO corresponding prev variables and mask\n",
        "        temp_content_stylized = st_ada_in(temp_style, temp_content, temp_content_prev_3, alpha = 0.1)\n",
        "        temp_content_prev_3 = temp_content.clone().detach()\n",
        "        temp_content_stylized = temp_content_stylized + self.sb_3_short(st_ada_in_outputs[2])\n",
        "        temp_content_stylized = self.sb_32(temp_content_stylized)\n",
        "        temp_content_stylized = self.sb_33(temp_content_stylized) #c_out\n",
        "        temp_style = self.sb_33(temp_style) #s_out\n",
        "\n",
        "        temp_content_stylized = self.l_1(temp_content_stylized)\n",
        "        temp_content_stylized = self.l_2(temp_content_stylized) #stylized_features\n",
        "\n",
        "        #grid prediction\n",
        "        bilateral_grid = self.gp_1(temp_content_stylized)\n",
        "        bilateral_grid = self.gp_2(bilateral_grid)\n",
        "\n",
        "        content_prev_encoded = content_encoded.copy()\n",
        "\n",
        "        return bilateral_grid, content_prev_encoded, temp_content_prev_1, temp_content_prev_2, temp_content_prev_3\n",
        "\n",
        "\n",
        "class SoftGridMask(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SoftGridMask, self).__init__()\n",
        "\n",
        "    def create_soft_grid_mask(self, z, M_px, w, h, W, H, D):\n",
        "        #TODO check reduce_sum against algo\n",
        "        M_grid = torch.tensor(np.zeros((D, W, H)), dtype=torch.float32)\n",
        "        print((z * M_px * D).shape)\n",
        "        z_D = torch.floor((z * M_px) * D)\n",
        "        s_w, s_h = w//W, h//H\n",
        "\n",
        "        for x in range(W):\n",
        "            for y in range(H):\n",
        "                patch = z_D[x * s_w : (x + 1) * s_w, y*s_h:(y+1)*s_h]\n",
        "                M_grid[:,x,y] = torch.sum(torch.where(patch > 0, patch, torch.zeros_like(patch)))\n",
        "                for d in range(D):\n",
        "                    if torch.any(patch == d):\n",
        "                        M_grid[d,x,y] = torch.sum(torch.where(patch == d, patch, torch.zeros_like(patch)))\n",
        "        M_grid = M_grid / (s_w * s_h)\n",
        "        return M_grid\n",
        "\n",
        "    def forward(self, z, M_px, w, h, W, H, D):\n",
        "        #TODO : use create mask function\n",
        "        return self.create_soft_grid_mask(z, M_px, w, h, W, H, D)\n",
        "\n",
        "\n",
        "class GridBlender(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GridBlender, self).__init__()\n",
        "\n",
        "    def forward(self, grids, soft_grid_mask):\n",
        "        #TODO : use grid blending function\n",
        "        out = torch.zeros_like(grids[0])\n",
        "        for g in grids:\n",
        "            out = torch.add(out, g)\n",
        "        return out\n",
        "#########################################################################################################\n",
        "class MaskedVideoStyleTransfer(nn.Module):\n",
        "    def __init__(self, spatio_temporal_model, grid_blender, guide, slice, apply_coeffs, soft_grid_mask):\n",
        "        super(MaskedVideoStyleTransfer, self).__init__()\n",
        "        self.spatio_temporal_model = spatio_temporal_model\n",
        "        self.grid_blender = grid_blender\n",
        "        self.guide = guide\n",
        "        self.slice = slice\n",
        "        self.apply_coeffs = apply_coeffs\n",
        "        self.soft_grid_mask = soft_grid_mask\n",
        "\n",
        "\n",
        "    def forward(self, content, mask_cur_list, style_list, content_prev, mask_prev_list, content_prev_encoded_list, temp_content_prev_1_list, temp_content_prev_2_list, temp_content_prev_3_list):\n",
        "        # content, mask_cur_list, style_list, content_prev, mask_prev_list = inputs\n",
        "        print(content.shape, len(mask_cur_list), len(style_list), content_prev.shape, len(mask_prev_list), len(content_prev_encoded_list), len(temp_content_prev_1_list), len(temp_content_prev_2_list), len(temp_content_prev_3_list))\n",
        "\n",
        "        # #prev values\n",
        "        # content_prev_encoded_list, temp_content_prev_1_list, temp_content_prev_2_list, temp_content_prev_3_list = None, None, None, None\n",
        "\n",
        "        bilateral_grid_list = []\n",
        "        for classId in range(len(mask_cur_list)):\n",
        "            bilateral_grid, content_prev_encoded, temp_content_prev_1, temp_content_prev_2, temp_content_prev_3 = self.spatio_temporal_model(style_list[classId], content, mask_cur_list[classId], content_prev_encoded_list[classId], temp_content_prev_1_list[classId], temp_content_prev_2_list[classId], temp_content_prev_3_list[classId], mask_prev_list[classId])\n",
        "\n",
        "            content_prev_encoded_list[classId], temp_content_prev_1_list[classId], temp_content_prev_2_list[classId], temp_content_prev_3_list[classId] = content_prev_encoded, temp_content_prev_1, temp_content_prev_2, temp_content_prev_3\n",
        "\n",
        "            bilateral_grid_list.append(bilateral_grid)\n",
        "\n",
        "        guide = self.guide(content)\n",
        "        soft_grid_mask = self.soft_grid_mask(guide, mask_cur_list[0], 256, 256, 32, 32, 96)\n",
        "        #TODO: handle multiple masks, one for each class\n",
        "        blended_grid = self.grid_blender(bilateral_grid_list, soft_grid_mask)\n",
        "        blended_grid = blended_grid.reshape(blended_grid.shape[0],12,-1,blended_grid.shape[-2],blended_grid.shape[-1])\n",
        "        slice_coeffs = self.slice(blended_grid, guide)\n",
        "        output = self.apply_coeffs(slice_coeffs, content)\n",
        "\n",
        "        #TODO : loss calculation, gradient calculation, backpropagation\n",
        "        return output\n",
        "########################################################################################################\n",
        "\n",
        "# class LaplacianRegularizer2D(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(LaplacianRegularizer, self).__init__()\n",
        "#         self.mse_loss = torch.nn.MSELoss(reduction='sum')\n",
        "#     def forward(self,f):\n",
        "#         loss = 0.\n",
        "#         for i in range(f.shape[2]):\n",
        "#             for j in range(f.shape[3]):\n",
        "#                 up = max(i-1,0)\n",
        "#                 down = min(i+1,f.shape[2] - 1)\n",
        "#                 left = max(j-1,0)\n",
        "#                 right = min(j+1,f.shape[3] - 1)\n",
        "#                 term = f[:,:,i,j].view(f.shape[0],f.shape[1],1,1).expand(f.shape[0],f.shape[1],down - up+1,right-left+1)\n",
        "#                 loss += self.mse_loss(term,f[:,:,up:down+1,left:right+1])\n",
        "#         return loss\n",
        "\n",
        "# class LaplacianRegularizer3D(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(LaplacianRegularizer3D, self).__init__()\n",
        "#         self.mse_loss = torch.nn.MSELoss(reduction='sum')\n",
        "\n",
        "#     def forward(self, f):\n",
        "#         loss = 0.\n",
        "#         # f: [B, 12, 8, H, W]\n",
        "#         f = f.reshape(f.shape[0],12,8,f.shape[2],f.shape[-1])\n",
        "#         B, C, D, H, W = f.shape\n",
        "#         for k in range(D):\n",
        "#             for i in range(H):\n",
        "#                 for j in range(W):\n",
        "#                     front = max(k - 1, 0)\n",
        "#                     back = min(k + 1, D - 1)\n",
        "#                     up = max(i - 1, 0)\n",
        "#                     down = min(i + 1, H - 1)\n",
        "#                     left = max(j - 1, 0)\n",
        "#                     right = min(j + 1, W - 1)\n",
        "#                     term = f[:, :, k, i, j].view(B, C, 1, 1, 1).expand(B, C, back - front + 1, down - up + 1, right - left + 1)\n",
        "#                     loss += self.mse_loss(term, f[:, :, front:back + 1, up:down + 1, left:right + 1])\n",
        "#         return loss\n",
        "\n",
        "# # true laplacian_regularizer of original paper, input weight is coeffs in 5-dimension form\n",
        "# def calc_laplacian_regularizer_loss(self, weights, l1=0.0, l2=0.0):\n",
        "#         if not l1 and not l2:\n",
        "#             return 0.0\n",
        "#         diff1 = weights[:, :, 1:, :, :] - weights[:, :, :-1, :, :]\n",
        "#         diff2 = weights[:, :, :, 1:, :] - weights[:, :, :, :-1, :]\n",
        "#         diff3 = weights[:, :, :, :, 1:] - weights[:, :, :, :, :-1]\n",
        "#         if l1:\n",
        "#             result1 = torch.abs(diff1).sum()\n",
        "#             result1 += torch.abs(diff2).sum()\n",
        "#             result1 += torch.abs(diff3).sum()\n",
        "#         if l2:\n",
        "#             result2 = torch.pow(diff1, 2).sum()\n",
        "#             result2 += torch.pow(diff2, 2).sum()\n",
        "#             result2 += torch.pow(diff3, 2).sum()\n",
        "#         if l1 and not l2:\n",
        "#             return result1\n",
        "#         elif not l1 and l2:\n",
        "#             return result2\n",
        "#         else:\n",
        "#             return result1 + result2\n",
        "\n",
        "class Slice(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Slice, self).__init__()\n",
        "\n",
        "    def forward(self, bilateral_grid, guidemap):\n",
        "        device = bilateral_grid.get_device()\n",
        "\n",
        "        N, _, H, W = guidemap.shape\n",
        "        hg, wg = torch.meshgrid([torch.arange(0, H), torch.arange(0, W)]) # [0,511] HxW\n",
        "        if device >= 0:\n",
        "            hg = hg.to(device)\n",
        "            wg = wg.to(device)\n",
        "        hg = hg.float().repeat(N, 1, 1).unsqueeze(3) / (H-1) * 2 - 1 # norm to [-1,1] NxHxWx1\n",
        "        wg = wg.float().repeat(N, 1, 1).unsqueeze(3) / (W-1) * 2 - 1 # norm to [-1,1] NxHxWx1\n",
        "        guidemap = guidemap.permute(0,2,3,1).contiguous()\n",
        "        guidemap_guide = torch.cat([wg, hg, guidemap], dim=3).unsqueeze(1) # Nx1xHxWx3\n",
        "        coeff = F.grid_sample(bilateral_grid, guidemap_guide)\n",
        "        return coeff.squeeze(2)\n",
        "\n",
        "class ApplyCoeffs(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ApplyCoeffs, self).__init__()\n",
        "        self.degree = 3\n",
        "\n",
        "    def forward(self, coeff, full_res_input):\n",
        "\n",
        "        '''\n",
        "            Affine:\n",
        "            r = a11*r + a12*g + a13*b + a14\n",
        "            g = a21*r + a22*g + a23*b + a24\n",
        "            ...\n",
        "        '''\n",
        "\n",
        "        R = torch.sum(full_res_input * coeff[:, 0:3, :, :], dim=1, keepdim=True) + coeff[:, 3:4, :, :]\n",
        "        G = torch.sum(full_res_input * coeff[:, 4:7, :, :], dim=1, keepdim=True) + coeff[:, 7:8, :, :]\n",
        "        B = torch.sum(full_res_input * coeff[:, 8:11, :, :], dim=1, keepdim=True) + coeff[:, 11:12, :, :]\n",
        "\n",
        "        return torch.cat([R, G, B], dim=1)\n",
        "\n",
        "class GuideNN(nn.Module):\n",
        "    def __init__(self, params=None):\n",
        "        super(GuideNN, self).__init__()\n",
        "        self.params = params\n",
        "        self.conv1 = ConvBlock(3, 16, kernel_size=1, padding=0, batch_norm=False)\n",
        "        self.conv2 = ConvBlock(16, 1, kernel_size=1, padding=0, activation=nn.Tanh)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv2(self.conv1(x))\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=\"same\", stride=1, use_bias=True, activation=nn.ReLU,\n",
        "                 batch_norm=False):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        reflection_padding = kernel_size // 2 # same dimension after padding\n",
        "        self.reflection_pad = nn.ReflectionPad2d(reflection_padding)\n",
        "        self.conv = nn.Conv2d(int(in_channels), int(out_channels), kernel_size, stride=stride, bias=use_bias)\n",
        "        self.activation = activation() if activation else None\n",
        "        self.bn = nn.BatchNorm2d(out_channels) if batch_norm else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.reflection_pad(x)\n",
        "        x = self.conv(x)\n",
        "        if self.bn:\n",
        "            x = self.bn(x)\n",
        "        if self.activation:\n",
        "            x = self.activation(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "LkPrVDa2HtbV"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stm = SpatioTemporalTransferModel()\n",
        "V = 256\n",
        "D = 224\n",
        "temp = np.ones((1,3,V,V))\n",
        "# print(temp.shape)\n",
        "a,b,c,d,e = stm.forward(torch.tensor(temp, dtype=torch.float32), torch.tensor(temp, dtype=torch.float32),\n",
        "            torch.tensor(np.ones((1,1,V,V)), dtype=torch.float32), [torch.tensor(np.zeros((1, 64, 224, 224)), dtype=torch.float32), torch.tensor(np.zeros((1, 128, 112, 112)), dtype=torch.float32), torch.tensor(np.zeros((1, 256, 56, 56)), dtype=torch.float32), torch.tensor(np.zeros((1, 512, 28, 28)), dtype=torch.float32)],\n",
        "            torch.tensor(np.zeros((1, 8, 112, 112)), dtype=torch.float32), torch.tensor(np.zeros((1, 16, 56, 56)), dtype=torch.float32),\n",
        "            torch.tensor(np.zeros((1, 32, 28, 28)), dtype=torch.float32), torch.tensor(np.ones((1,1,V,V)), dtype=torch.float32))\n",
        "print(a.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpbEVkoLuvZN",
        "outputId": "1205cc07-714e-49d9-e608-077c4ceb91a1"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 96, 16, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g = GuideNN()\n",
        "z = g(torch.tensor(np.zeros((1, 3, 256, 256)), dtype=torch.float32))\n",
        "print(z.shape)"
      ],
      "metadata": {
        "id": "S0rDqsPENZwq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "040e0ca7-bec4-45d8-f98f-a2202bf2b87c"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 256, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sgm = SoftGridMask()\n",
        "sm = sgm.create_soft_grid_mask(z, torch.tensor(np.zeros((1, 1, 256, 256)), dtype=torch.float32), 256, 256, 16, 16, 96)\n",
        "print(sm.shape)"
      ],
      "metadata": {
        "id": "RMPa3cz_pEQ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "813ae02e-7e1b-486e-e951-b29bcc7290e1"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 256, 256])\n",
            "torch.Size([96, 16, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "slice = Slice()\n",
        "slice_coeffs = slice(a.reshape(a.shape[0],12,-1,a.shape[-2],a.shape[-1]), z)\n",
        "print(slice_coeffs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBJh_KzAUjQs",
        "outputId": "89f48875-e8ee-486c-e2a9-c81b72936896"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 12, 256, 256])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4316: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "apply = ApplyCoeffs()\n",
        "apply_coeffs = apply(slice_coeffs, torch.tensor(temp, dtype=torch.float32))\n",
        "print(apply_coeffs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UyE5fXOoEOG",
        "outputId": "dac433ec-44b8-467e-fe29-5252cc9a52a2"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 256, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gb = GridBlender()\n",
        "bg = gb([torch.tensor(np.zeros((1, 12, 8, 16, 16)), dtype=torch.float32), torch.tensor(np.ones((1, 12, 8, 16, 16)), dtype=torch.float32)], torch.tensor(np.zeros((1, 12, 8, 16, 16)), dtype=torch.float32))\n",
        "print(bg.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vJvHqJDCYu2",
        "outputId": "bbc165ad-b8b9-405f-afa0-d5983b5d0580"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 12, 8, 16, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mvs = MaskedVideoStyleTransfer(stm, GridBlender(), g, slice, apply, sgm)\n",
        "out = mvs(torch.tensor(temp, dtype=torch.float32), [torch.tensor(temp, dtype=torch.float32)], [torch.tensor(temp, dtype=torch.float32)], torch.tensor(temp, dtype=torch.float32), [torch.tensor(temp, dtype=torch.float32)], [[torch.tensor(np.zeros((1, 64, 224, 224)), dtype=torch.float32), torch.tensor(np.zeros((1, 128, 112, 112)), dtype=torch.float32), torch.tensor(np.zeros((1, 256, 56, 56)), dtype=torch.float32), torch.tensor(np.zeros((1, 512, 28, 28)), dtype=torch.float32)]],\n",
        "            [torch.tensor(np.zeros((1, 8, 112, 112)), dtype=torch.float32)], [torch.tensor(np.zeros((1, 16, 56, 56)), dtype=torch.float32)],\n",
        "            [torch.tensor(np.zeros((1, 32, 28, 28)), dtype=torch.float32) ])\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0z1UCdST6kfi",
        "outputId": "8c02f748-95ef-436b-a89c-107b2c751db6"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 256, 256]) 1 1 torch.Size([1, 3, 256, 256]) 1 1 1 1 1\n",
            "torch.Size([1, 3, 256, 256])\n",
            "torch.Size([1, 3, 256, 256])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4316: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # training steps :\n",
        "# content_prev = None\n",
        "# style_list = [] #set style for each class\n",
        "# content_prev_encoded_list, temp_content_prev_1_list, temp_content_prev_2_list, temp_content_prev_3_list = None, None, None, None\n",
        "# for content in video_frames:\n",
        "#     mask_list = get_masks_for_frame(content)\n",
        "#     improved_mask_list = improve_masks(mask_list, content)\n",
        "#     train_step(content, improved_mask_list, content_prev, mask_prev_list, style_list, content_prev_encoded_list, temp_content_prev_1_list, temp_content_prev_2_list, temp_content_prev_3_list)\n",
        "#     content_prev = content.copy()\n",
        "#     mask_prev_list = improved_mask_list.copy()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Hl2aydzt4NKB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}